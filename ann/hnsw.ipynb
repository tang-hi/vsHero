{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HNSW(Hierarchical Navigable Small World)\n",
    "HNSW是通过图的方式来解决向量搜索问题的算法，由Y.Malkov与D.Yashunin在[论文](https://arxiv.org/pdf/1603.09320.pdf)中首次提出。\n",
    "\n",
    "这一个Section安排如下\n",
    "1. 图拥有什么样的性质可以有效的找到最近的K个向量\n",
    "2. NSW(Navigable Small World)\n",
    "3. HNSW(Hierarchical Navigable Small World) \n",
    "\n",
    "### 1. 图的性质\n",
    "我们先直观的感受一下使用图的方式来表现向量空间。\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/raw_vector.png\"/>\n",
    "</div>\n",
    "\n",
    "图中的点代表向量，我们可以看到，如果两个向量的距离较近，那么在图中这两个点之间的距离也会更近。当我们想要通过图的方式来解决向量搜索时，我们会希望从任一点出发可以到达图中其他所有的点，即这个图是一张联通图。\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/compare_vec.png\"/>\n",
    "</div>\n",
    "\n",
    "但仅仅只是联通图，仍然无法做到快速有效的找到距离最近的K个向量。考虑如下的情况，A点与B点之间相隔较远，因此如果想要从A点到达B点需要途经许多点(代表着大量的计算)，同时我们可以看到点C与许多其他的点都有连接，因此如果我们从点C开始寻找距离查询向量最近的K个点，我们会计算大量无关的点(因为与点C相连的点，其中很多大概率是与结果无关的)。\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/two_conn.png\"/>\n",
    "</div>\n",
    "\n",
    "综上所述，为了可以高效而准确的找到距离查询向量最近的K个向量。我们希望构建的图有以下几个性质\n",
    "1. 联通图(没有孤岛)\n",
    "2. 距离较远的点,有边可以相连(long range edge)\n",
    "3. 构建的图中边的数量不宜过多(大量的计算)\n",
    "4. 距离相近的点，有边连接（保证召回率）\n",
    "其中3,4是召回率与计算量的tradeoff。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NSW\n",
    "NSW通过有效且简单的算法构建出满足上述要求的图，下面分别从构建以及查询两个方面来介绍NSW算法。\n",
    "\n",
    "#### 1. 构建\n",
    "首先我们将通过随机的方式，将向量一个一个添加到图中，每一个新添加的点都会与当前图中距离该点最近的**M**个点相连。之所以通过**M**对相连的点的数量进行限制，是为了防止连接的边过多，从而影响查询效率。\n",
    "\n",
    "我们通过一个例子来描述构建的过程，假设我们将**M**设置为3，并且已经将待加入的向量随机打乱。\n",
    "\n",
    "首先，添加点A，因为当前图中没有其他任何的点，所以我们只需要添加A，而不用作任何其他的操作。后面我们继续添加点B，此时图中只有点A，点的个数小于3,因此我们可以直接将两者相连。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/addB.png\"/>\n",
    "</div>\n",
    "\n",
    "类似的我们向图中加入点C，点D，我们会获得以下的图\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/addCD.png\"/>\n",
    "</div>\n",
    "\n",
    "随后我们继续添加点E，此时我们会找到当前图中距离点E最近的**M**个点，即A，B，C并将其相互连接。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/addE.png\"/>\n",
    "</div>\n",
    "\n",
    "用相同的方式，我们继续添加点F，G，H，最终得到的图如下所示。\n",
    "\n",
    "我们逐个检查NSW图是否满足我们之前要求的性质\n",
    "1. 联通图，显而易见这是一张联通图\n",
    "2. 距离较远的点有边可以相连，我们可以发现因为随机添加，最开始认为距离较近的点，比如A，D，随着添加的点越来越多，A，D相连的这条边成为了一条long range边。\n",
    "3. 构建的图中，边的数量不宜过多。这一条因为我们始终用**M**控制边的数量，所以也可以满足\n",
    "4. 距离较近的点，有边连接。因为我们始终与距离最近的**M**条边相连，因此也满足了该要求。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/addFH.png\"/>\n",
    "</div>\n",
    "\n",
    "因此，我们只需要随机添加向量，并且在随机添加的过程中，与当前最近的**M**个点相连，我们就可以构建出一幅可以高效的进行ANN查询的图。下面我们讨论搜索的过程。\n",
    "\n",
    "#### 2. 搜索\n",
    "\n",
    "因为我们通过NSW构建出的图,具有良好的特性,因此我们只需要使用简单的贪心算法就可以获得较好的搜索结果。在给定了一个query point后\n",
    "1. 我们在图中随机的选择一个点作为出发点(entry point)\n",
    "2. 我们计算每一个与该点相连的点，选出最近的一个点。\n",
    "    \n",
    "    a. 若该点即为entry point,搜索结束,返回entry point。\n",
    "    \n",
    "    b. 若该点不为entry point, 设置该点为entry point, 重复过程2\n",
    "\n",
    "下图为搜索的示意图,我们可以看到因为有long range，这一高速通道的存在，我们可以快速搜索到结果。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/nsw-search.png\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.HNSW\n",
    "尽管NSW已经可以很好的为我们解决ANN查询的问题，但其仍然有不足之处。\n",
    "1. 搜索时，NSW无法区分long range与short range，从而无法先查询long range再查询short range。\n",
    "2. 当数据的聚类效应特别明显时，即使我们乱序加入向量，cluster之间相互连接的边仍然十分稀疏，从而搜索结果容易陷入局部最优，同时效率也会比较低下。\n",
    "\n",
    "因此为了解决上述问题，HNSW作为NSW的改良版被提了出来。\n",
    "\n",
    "#### 1.构建\n",
    "我们首先直观的感受HNSW图。我们可以看到hnsw相比于nsw多了层级的概念。我们从图中可以看到，level0中有全部的向量，随着层数的增加，向量的数量也相应的减少。\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/hnsw.png\"/>\n",
    "</div>\n",
    "\n",
    "HNSW并不要求我们乱序插入向量，当我们向HNSW添加新的向量时，我们首先会通过一个指数衰减的概率函数，得到这个向量所处的最大层级(如果最大层级计算出来是3,那么level3, level2,level1,level0中都含有这个向量)。\n",
    "\n",
    "这就意味着，绝大多数的向量所处的最高层级都是level0, 同样我们也可以认为高层级是低层级的草图（抽样），因此高层级中的向量之间大概率是long range连接，低层级中的向量则是short range连接。这样子做给我们带来的好处就是搜索时，我们可以先寻找long range的边，再寻找short range的边，即先粗查再精查。从而尽可能减少搜索的次数。\n",
    "\n",
    "当得到这个向量所处的最大层级后，我们便需要将其添加到图中。假设，新增的向量为**V**, 这个向量所处的最大层级为**I**,HNSW的最高层级为**J**。添加向量时因为需要从最高层级**J**，一直走到最低层级0,我们将添加时所处的层级设置为**C**。添加的过程可以分为3个阶段\n",
    "\n",
    "1. **J** >= **C** > **I**\n",
    "    这一阶段我们使用NSW的贪心算法，寻找距离最近的向量，随后在下一层级以这个点为搜索起点.\n",
    "2. **I** >= **C** > 0\n",
    "    在这一阶段，我们不仅要找距离最近的向量，我们还需要将**V**存放到这一级的图中。我们仍旧使用贪心算法寻找距离最近的向量，不同之处在于我们会维护一个动态的列表，保存距离**V**最近的`efCounstruction`个向量，`efConstruction`为可调节的参数。当我们在这一层搜索完成后，我们会将这一动态列表作为`Candidate`,并从中取出`M`个向量，与其连接。\n",
    "3. **I** = 0\n",
    "    这一阶段，我们采用和第二阶段一样的策略，不同的是在level0,向量**V**可以与最多**2M**个向量进行连接。\n",
    "\n",
    "下图为一个简单的示例，新增向量所处的最高层级为1。我们首先在level2中，寻找与其最近的点(黄色标示)，找到后，我们以这个点为起点在level1中寻找与其最近的`efCounstruction`个点，随后与其中的**M**个向量进行连接。最后当我们到达level0时，我们用上一层连接的**M**个向量作为起点寻找符合要求的**2M**个向量，并与其相连。\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/new-insert.png\"/>\n",
    "</div>\n",
    "\n",
    "当我们在某一层中(**I** >= **C** >= 0)找到距离**V**最近的`efConstruction`向量后，我们需要从中挑选出**M**个向量用以与**V**连接。\n",
    "一种简单的做法是直接从`efCounstruction`中挑选出最近的**M**个向量，但是这种做法当数据的聚类效果特别明显时，会导致不同cluster之间的连接十分稀疏，导致搜索陷入局部最优，并且查询效率降低。\n",
    "\n",
    "因此我们采用启发式的搜索方式，假定我们新增的向量为**V**，挑选出的`efConstruction`个向量为**Candidate**,当前我们已经选择出需要连接的向量为**Result**,启发式的算法为\n",
    "\n",
    "```python\n",
    "while len(Candidate) > 0 and len(Result) < M:\n",
    "    c = pop nearest element from Candidate to V\n",
    "    for r in Result:\n",
    "        lowest = min(lowest, distance(r, c))\n",
    "    if dis(c, V) < lowest:\n",
    "        Result += c\n",
    "```\n",
    "\n",
    "用一张图来描述这种情况,我们从C1,C2中决定哪个点应该作为下一个连接点时，我们会选择与`inserted`之间的距离相比与其他`result`更近的点，而不是距离`inserted`更近的点。\n",
    "按照论文中的说法，这可以帮助我们在高度聚类的数据中，取得更好的搜索效果以及效率。\n",
    "> The heuristic enhances the diversity of a vertex’s neighborhood and leads to better search efficiency for the case of highly clustered data.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/her.png\"/>\n",
    "</div>\n",
    "\n",
    "#### 2.搜索\n",
    "搜索的过程分为两个阶段\n",
    "1. **J** >= **C** > 0\n",
    "    这一阶段我们使用NSW的贪心算法，在这一层中寻找距离最近的向量，随后在下一层级以这个点为搜索起点继续搜索.\n",
    "2. **I** = 0\n",
    "    这一阶段，我们仍旧使用贪心的搜索策略，不同之处在于，我们会维护一个距离最近的`efSearch`个向量，并最终返回结果。\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"./resources/search.png\"/>\n",
    "</div>\n",
    "\n",
    "#### 3.Summary\n",
    "HNSW,通过引入层级的概念以及启发式搜索，解决了搜索时，NSW无法区分long range与short range，以及面对高度聚集的数据时，搜索效率的低下。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab\n",
    "\n",
    "在这个Lab中你需要先阅读[HNSW](https://arxiv.org/pdf/1603.09320.pdf)的Section3, Section4,然后实现HNSW算法，当你完成后，我们会使用siftsmall数据集进行测试，你应当取得krecall > 90%, avg-time < 0.01s 的结果。\n",
    "#### HINT\n",
    "1. 你需要在HNSW框架代码中实现\n",
    "    - `add`,point为向量，label为向量ID(每一个向量ID唯一)。\n",
    "    - `search`, query为向量，efSearch为需要返回的最近的K个向量。\n",
    "2. 我们已经为你实现了`__generateLevel`, `__distance`\n",
    "3. 请不要更改`add`以及`search`的函数签名。\n",
    "4. 我们为你提供了`__init__`,`__search_layer`,`__select_neighbors`等函数框架，你可以按照自己的意愿任意实现或修改。\n",
    "5. 我们为你提供了`PriorityQueue`,也许你在后续实现中会需要它:)\n",
    "    ```python\n",
    "    w = PriorityQueue(reversed=True)\n",
    "    w.push((1,3)) #push (1,3) \n",
    "    a,b = w.top() # a = 1, b = 3\n",
    "    w.pop()       #pop(1,3) from w\n",
    "    ```\n",
    "\n",
    "> krecall 表示topK返回的结果中有多少比例的向量属于KNN的topK返回的结果。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "如果你已经下载了数据，可以直接跳过这个cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import urllib.request as request\n",
    "import tarfile\n",
    "from contextlib import closing\n",
    "\n",
    "# download the Sift1M dataset\n",
    "with closing(request.urlopen('ftp://ftp.irisa.fr/local/texmex/corpus/siftsmall.tar.gz')) as r:\n",
    "    with open('sift.tar.gz', 'wb') as f:\n",
    "        shutil.copyfileobj(r, f)\n",
    "\n",
    "\n",
    "# the download leaves us with a tar.gz file, we unzip it\n",
    "tar = tarfile.open('sift.tar.gz', \"r:gz\")\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "class PriorityQueue:\n",
    "    \"\"\"\n",
    "    PriorityQueue is a class that implements a priority queue using a heap.\n",
    "\n",
    "    Attributes:\n",
    "        reversed (bool): A boolean indicating whether the queue is reversed.\n",
    "        pq (list): A list containing the heap.\n",
    "        count (int): The number of elements in the queue.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of elements in the queue.\n",
    "        push(item): Pushes an item onto the queue.\n",
    "        pop(): Pops an item from the queue.\n",
    "        top(): Returns the top element of the queue.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reversed=False):\n",
    "        self.reversed = reversed\n",
    "        self.pq = []\n",
    "        self.count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def push(self, item):\n",
    "        \"\"\"\n",
    "        Pushes an item onto the queue.\n",
    "\n",
    "        Args:\n",
    "            item (tuple): The item to be pushed onto the queue.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.count += 1\n",
    "        if self.reversed:\n",
    "            heapq.heappush(self.pq, (-item[0], self.count, item[1]))\n",
    "            return\n",
    "        heapq.heappush(self.pq, (item[0], self.count, item[1]))\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        Pops an item from the queue.\n",
    "\n",
    "        Returns:\n",
    "            int: The popped item.\n",
    "        \"\"\"\n",
    "        self.count -= 1\n",
    "        if self.reversed:\n",
    "            item = heapq.heappop(self.pq)\n",
    "            return -1.0 * item[0], item[2]\n",
    "\n",
    "        item = heapq.heappop(self.pq)\n",
    "        return item[0], item[2]\n",
    "\n",
    "    def top(self):\n",
    "        \"\"\"\n",
    "        Returns the top element of the queue.\n",
    "\n",
    "        Returns:\n",
    "            int: The top element of the queue.\n",
    "        \"\"\"\n",
    "        if self.reversed:\n",
    "            return -1.0 * self.pq[0][0], self.pq[0][2]\n",
    "\n",
    "        return self.pq[0][0], self.pq[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "class HNSW:\n",
    "    def __init__(self, dim, M, efConstruction) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the HNSW object with the given dimension, M (maximum number of neighbors),\n",
    "        and efConstruction (size of the dynamic list during graph construction).\n",
    "        \"\"\"\n",
    "        self.datas = {}\n",
    "        self.enterPoint = -1\n",
    "        self.maxLevel = -1\n",
    "        self.dim = dim\n",
    "        self.efConstruction = efConstruction\n",
    "        self.edges = {}\n",
    "        self.Mmax = M\n",
    "        self.Mmax0 = 2 * M\n",
    "        self.levelMult = 1 / np.log(1.0 * M)\n",
    "\n",
    "    def add(self, point: np.ndarray, label: int) -> None:\n",
    "        \"\"\"\n",
    "        Adds a point to the HNSW graph with a given label, connecting it to neighbors\n",
    "        and updating neighbor connections if necessary.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def search(self, query: np.ndarray, efSearch: int) -> list:\n",
    "        \"\"\"\n",
    "        Performs a top-down search through the graph layers to find the approximate\n",
    "        nearest neighbors of a query point.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __distance(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the Euclidean distance between two points.\n",
    "\n",
    "        Args:\n",
    "            vec1 (np.ndarray): The first point.\n",
    "            vec2 (np.ndarray): The second point.\n",
    "\n",
    "        Returns:\n",
    "            float: The Euclidean distance between vec1 and vec2.\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "    def __generateLevel(self) -> int:\n",
    "        \"\"\"\n",
    "        Generates the level for a new point based on a geometric distribution.\n",
    "\n",
    "        Returns:\n",
    "            int: The generated level for the new point.\n",
    "        \"\"\"\n",
    "        distribution = np.random.uniform(0.0, 1.0)\n",
    "        r = -np.log(distribution) * self.levelMult\n",
    "        return int(r)\n",
    "\n",
    "    def __search_layer(self, query: np.ndarray, eps: set, layer: int, efSearch: int) -> PriorityQueue:\n",
    "        \"\"\"\n",
    "        Searches for the nearest neighbors in a specific layer of the HNSW graph.\n",
    "\n",
    "        Args:\n",
    "            query (np.ndarray): The query point.\n",
    "            eps (set): The entry points for the search.\n",
    "            layer (int): The layer to search in.\n",
    "            efSearch (int): The size of the dynamic list during search.\n",
    "\n",
    "        Returns:\n",
    "            PriorityQueue: The approximate nearest neighbors found in the layer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __select_neighbors(self, label: int, candidates: PriorityQueue, M: int) -> set:\n",
    "        \"\"\"\n",
    "        Selects neighbors for a new point considering the candidate neighbors.\n",
    "\n",
    "        Args:\n",
    "            label (int): The label of the new point.\n",
    "            candidates (PriorityQueue): The candidate neighbors.\n",
    "            M (int): The maximum number of neighbors to select.\n",
    "\n",
    "        Returns:\n",
    "            set: The selected neighbors.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def read_fvecs(file: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a file containing fvecs and returns a numpy array.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to the file containing fvecs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing the fvecs.\n",
    "    \"\"\"\n",
    "    fv = np.fromfile(file, dtype=np.float32)\n",
    "    if fv.size == 0:\n",
    "        return np.zeros((0, 0))\n",
    "    dim = fv.view(np.int32)[0]\n",
    "    return fv.reshape(-1, dim + 1)[:, 1:].copy().view('float32')\n",
    "\n",
    "\n",
    "def read_ivecs(file: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads an ivecs file and returns a numpy array.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to the ivecs file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Numpy array containing the data from the ivecs file.\n",
    "    \"\"\"\n",
    "    ground_truth = np.fromfile(file, dtype=np.int32)\n",
    "    if ground_truth.size == 0:\n",
    "        return ground_truth.zeros((0, 0))\n",
    "    d = ground_truth[0]\n",
    "    return ground_truth.reshape(-1, d+1)[:, 1:].copy().view('int32')\n",
    "\n",
    "\n",
    "# load dataset\n",
    "data_set = read_fvecs('./siftsmall/siftsmall_base.fvecs')\n",
    "# load all query\n",
    "query = read_fvecs('./siftsmall/siftsmall_query.fvecs')\n",
    "\n",
    "\n",
    "ground_truth = np.fromfile(\n",
    "    \"./siftsmall/siftsmall_groundtruth.ivecs\", dtype=np.int32)\n",
    "d = ground_truth[0]\n",
    "gt = ground_truth.reshape(-1, d+1)[:, 1:].copy().view('int32')\n",
    "\n",
    "\n",
    "def intersection(lst1: list, lst2: list) -> list:\n",
    "    return set(lst1).intersection(lst2)\n",
    "\n",
    "\n",
    "def k_recall(hnsw, k=10) -> tuple:\n",
    "    recall = 0.0\n",
    "    st = time.time()\n",
    "    times = 0\n",
    "    for query_idx in range(0, len(query)):\n",
    "        times += 1\n",
    "        result = hnsw.search(query[query_idx], k)\n",
    "        recall += (len(intersection(result[:k], gt[query_idx][:k])) / k)\n",
    "        print(result)\n",
    "        print(gt[query_idx][:k])\n",
    "    se = time.time()\n",
    "    return (recall / times, (se - st) / times)\n",
    "\n",
    "\n",
    "hnsw = HNSW(128, 16, 100)\n",
    "for label in range(0, len(data_set)):\n",
    "    hnsw.add(data_set[label], label)\n",
    "\n",
    "recall, t = k_recall(hnsw, 10)\n",
    "print(f'hnsw recall {recall}, time {t}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
